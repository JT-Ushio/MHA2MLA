# general
seed: 42
max_steps: 2000
report_to: wandb
run_name: rope_v4_topk4_ae_v2_rank8_tanh_init_lr1e-3
save_strategy : steps
save_steps: 0.5
output_dir: ../checkpoints/rope_v4_topk4_ae_v2_rank8_tanh_init
overwrite_output_dir: true
logging_strategy: steps
logging_steps: 10
resume_from_checkpoint: null
per_device_train_batch_size: 8
remove_unused_columns: False
gradient_accumulation_steps: 4
bf16: true
# deepspeed: ../configs_hf/rope/ds_config.json
dataloader_drop_last: true
# optim
optim: adamw_torch
max_grad_norm: 1.0
learning_rate: 1.0e-3
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-8
weight_decay: 0.01
# lr scheduler
use_constant_with_warmup_decay_scheduler: true
lr_scheduler_kwargs: {"lr_decay_starting_step": 1000, "lr_decay_steps": 1000, "lr_decay_style": "1-sqrt", "lr_warmup_steps": 200, "lr_warmup_style": "linear", "min_decay_lr": 0}


# model
model_name_or_path: /home/binguo/data/models/HuggingFaceTB/SmolLM-135M
tokenizer_name_or_path: /home/binguo/data/models/HuggingFaceTB/SmolLM-135M
RoPE:
  partial_rope_version: 4  # 0 1 2 3 4 5
  top_k_rope_dim: 4
  last_k_rope_dim: 0
  uniform_start_point: 0
  uniform_step: 1
  qk_tensor_path: '../utils/qk_tensor_135M.pth'
  n_gqa_group: 3
AE:
  version: 2
  low_rank: 8
  activation_fn: tanh

# dataset
is_nanoset: true
dataset_folders:
  - ~/local/smollm1_corpus/fineweb-edu-dedup/
  - ~/local/smollm1_corpus/cosmopedia-v2/
  - ~/local/smollm1_corpus/python-edu/
  - ~/local/smollm1_corpus/open-web-math/
  - ~/local/smollm1_corpus/stackoverflow/
dataset_weights: # 各数据集的权重
  - 0.7
  - 0.15
  - 0.08
  - 0.06
  - 0.01
sequence_length: 2048
